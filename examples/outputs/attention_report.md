# Attention Analysis Report
**Text:** The quick brown fox jumps over the lazy dog
**Model:** gpt2
**Generated:** 2025-06-02 19:35:42.864795

## Model Information
- Layers: 12
- Heads per layer: 12
- Sequence length: 9

## Overall Statistics
- Mean attention: 0.1111
- Attention entropy: 7.8378
- Attention sparsity: 0.5660

## Key Findings
- Highest entropy layer: 1
- Sparsest layer: 9
- Predominant pattern: mixed

## Layer-by-Layer Analysis
### Layer 0
- Pattern type: local
- Entropy: 5.6113
- Sparsity: 0.5370
- Local attention: 61.37%
- Global attention: 19.62%

### Layer 1
- Pattern type: mixed
- Entropy: 5.6375
- Sparsity: 0.4794
- Local attention: 45.87%
- Global attention: 29.57%

### Layer 2
- Pattern type: mixed
- Entropy: 5.6372
- Sparsity: 0.4928
- Local attention: 49.55%
- Global attention: 23.04%

### Layer 3
- Pattern type: mixed
- Entropy: 5.4351
- Sparsity: 0.5576
- Local attention: 44.55%
- Global attention: 32.18%

### Layer 4
- Pattern type: mixed
- Entropy: 5.3140
- Sparsity: 0.5669
- Local attention: 44.60%
- Global attention: 32.26%

### Layer 5
- Pattern type: mixed
- Entropy: 5.2045
- Sparsity: 0.6348
- Local attention: 31.05%
- Global attention: 44.41%

### Layer 6
- Pattern type: mixed
- Entropy: 5.2880
- Sparsity: 0.5607
- Local attention: 32.17%
- Global attention: 43.12%

### Layer 7
- Pattern type: mixed
- Entropy: 5.1873
- Sparsity: 0.6049
- Local attention: 30.05%
- Global attention: 44.97%

### Layer 8
- Pattern type: mixed
- Entropy: 5.2583
- Sparsity: 0.5576
- Local attention: 31.88%
- Global attention: 43.18%

### Layer 9
- Pattern type: mixed
- Entropy: 5.1146
- Sparsity: 0.6698
- Local attention: 28.66%
- Global attention: 47.00%

### Layer 10
- Pattern type: mixed
- Entropy: 5.1534
- Sparsity: 0.6235
- Local attention: 29.20%
- Global attention: 46.06%

### Layer 11
- Pattern type: mixed
- Entropy: 5.3932
- Sparsity: 0.5072
- Local attention: 35.95%
- Global attention: 40.25%
