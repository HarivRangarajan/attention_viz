#!/usr/bin/env python3
"""
ATTRIEVAL Demo: Attention-guided Retrieval for Long-Context Reasoning

This example demonstrates the ATTRIEVAL algorithm from the paper:
"Attention Reveals More Than Tokens: Training-Free Long-Context Reasoning with Attention-guided Retrieval"

The algorithm leverages attention weights from Chain-of-Thought (CoT) tokens to retrieve 
relevant facts from long contexts and improve reasoning performance.
"""

import os
import sys
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Add parent directory to path for imports
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from attention_viz import (
    AttentionExtractor, 
    AttrievelRetriever, 
    AttrievelConfig,
    create_attrieval_demo,
    load_model_and_tokenizer
)

def create_synthetic_long_context():
    """
    Create a synthetic long context similar to the paper's Deduction benchmark.
    Contains multiple facts with implicit relationships.
    """
    
    # Simulate a long context with scattered facts
    context = """
    One of the special magic messages is: The price of oranges is 73 USD.
    
    Many fruits are available in the market today. The weather is sunny and perfect for shopping.
    People often prefer organic fruits over conventional ones for health reasons.
    
    One of the special magic messages is: The price of nectarines is 32 USD cheaper than the price of oranges.
    
    The local grocery store has been in business for over 20 years. They pride themselves on quality.
    Fresh fruits are delivered daily to ensure maximum freshness and nutritional value.
    
    One of the special magic messages is: The price of apples is 15 USD more expensive than the price of nectarines.
    
    Seasonal fruits are often the best choice due to their peak flavor and lower environmental impact.
    Vitamin C is abundant in citrus fruits and helps boost the immune system.
    
    One of the special magic messages is: The price of bananas is half the price of apples.
    
    Many customers prefer to buy fruits in bulk to save money and reduce packaging waste.
    The store also offers a loyalty program for frequent customers with special discounts.
    """
    
    question = "What is the price of bananas?"
    
    # Chain-of-Thought response that might miss some implicit facts
    cot_response = """
    To find the price of bananas, I need to trace through the relationships:
    
    1. The price of bananas is half the price of apples
    2. The price of apples is 15 USD more expensive than nectarines
    3. The price of nectarines is 32 USD cheaper than oranges
    4. The price of oranges is 73 USD
    
    Let me calculate:
    - Oranges: 73 USD
    - Nectarines: 73 - 32 = 41 USD  
    - Apples: 41 + 15 = 56 USD
    - Bananas: 56 / 2 = 28 USD
    
    Therefore, the price of bananas is 28 USD.
    """
    
    return context, question, cot_response


def demonstrate_attention_aggregation_steps():
    """
    Step-by-step demonstration of attention aggregation as described in the paper.
    """
    
    print("üöÄ ATTRIEVAL: Step-by-Step Attention Aggregation Demo")
    print("=" * 70)
    
    # Step 1: Create synthetic data
    print("\nüìù Step 1: Creating synthetic long-context reasoning problem...")
    context, question, cot_response = create_synthetic_long_context()
    
    print(f"Context length: {len(context)} characters")
    print(f"Question: {question}")
    print("Chain-of-Thought response generated.")
    
    # Step 2: Load model and initialize components
    print("\nüì¶ Step 2: Loading model and initializing ATTRIEVAL components...")
    model, tokenizer = load_model_and_tokenizer("gpt2")
    
    extractor = AttentionExtractor(model, tokenizer)
    config = AttrievelConfig(
        layer_fraction=0.25,  # Use last 25% of layers
        top_k=50,            # Top 50 tokens per CoT token
        frequency_threshold=0.99,  # Filter attention sinks
        min_fact_length=3,   # Minimum 3 tokens per fact
        max_facts=10,        # Retrieve top 10 facts
        cross_eval_tokens=10 # Use 10 tokens for cross-evaluation
    )
    retriever = AttrievelRetriever(extractor, config)
    
    print("‚úÖ Components initialized successfully")
    
    # Step 3: Run ATTRIEVAL algorithm
    print("\nüîç Step 3: Running ATTRIEVAL Algorithm...")
    print("-" * 50)
    
    result = retriever.retrieve_facts(
        context=context,
        question=question, 
        cot_response=cot_response,
        use_cross_evaluation=True
    )
    
    # Step 4: Analyze results
    print("\nüìä Step 4: Analyzing Results...")
    print("-" * 30)
    
    print(retriever.visualize_retrieved_facts(result))
    
    # Step 5: Detailed analysis
    print("\nüî¨ Step 5: Detailed Analysis...")
    print("-" * 30)
    
    attention_data = result['attention_data']
    aggregated_attention = result['aggregated_attention']
    
    print(f"üìè Attention matrix shape: {aggregated_attention.shape}")
    print(f"üéØ Number of layers used: {len(attention_data['attention_weights']) * config.layer_fraction:.0f}")
    print(f"üìã Total facts segmented: {len(result['all_facts'])}")
    print(f"üîß Facts after filtering: {len(result['filtered_facts'])}")
    print(f"üèÜ Facts retrieved: {len(result['retrieved_facts'])}")
    
    if result['retriever_tokens']:
        print(f"üîÄ Cross-evaluation tokens selected: {len(result['retriever_tokens'])}")
    
    return result


def visualize_attention_aggregation(result):
    """
    Create visualizations showing the attention aggregation process.
    """
    
    print("\nüìà Step 6: Creating Attention Visualizations...")
    print("-" * 40)
    
    aggregated_attention = result['aggregated_attention']
    retrieved_facts = result['retrieved_facts']
    
    # Create output directory
    output_dir = "examples/outputs"
    os.makedirs(output_dir, exist_ok=True)
    
    # 1. Attention heatmap
    plt.figure(figsize=(12, 8))
    
    # Take a subset for visualization (attention matrix can be large)
    subset_size = min(50, aggregated_attention.shape[0])
    attention_subset = aggregated_attention[:subset_size, :subset_size]
    
    sns.heatmap(
        attention_subset, 
        cmap='Blues',
        cbar_kws={'label': 'Attention Weight'}
    )
    
    plt.title('ATTRIEVAL: Aggregated Attention Weights\n(Last 25% layers, averaged across heads)')
    plt.xlabel('Input Token Position')
    plt.ylabel('Generated Token Position')
    plt.tight_layout()
    
    heatmap_path = f"{output_dir}/attrieval_attention_heatmap.png"
    plt.savefig(heatmap_path, dpi=300, bbox_inches='tight')
    print(f"‚úÖ Attention heatmap saved: {heatmap_path}")
    plt.close()
    
    # 2. Fact scores visualization  
    if retrieved_facts:
        fact_texts = [f"Fact {i+1}" for i in range(len(retrieved_facts))]
        fact_scores = [fact['attention_score'] for fact in retrieved_facts]
        
        plt.figure(figsize=(10, 6))
        bars = plt.bar(fact_texts, fact_scores, color='lightblue', edgecolor='navy')
        
        # Add value labels on bars
        for bar, score in zip(bars, fact_scores):
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width()/2., height + 0.001,
                    f'{score:.4f}', ha='center', va='bottom')
        
        plt.title('ATTRIEVAL: Retrieved Facts Attention Scores')
        plt.xlabel('Retrieved Facts')
        plt.ylabel('Attention Score')
        plt.xticks(rotation=45)
        plt.tight_layout()
        
        scores_path = f"{output_dir}/attrieval_fact_scores.png"
        plt.savefig(scores_path, dpi=300, bbox_inches='tight')
        print(f"‚úÖ Fact scores visualization saved: {scores_path}")
        plt.close()
    
    # 3. Algorithm flow diagram
    create_algorithm_flow_diagram(output_dir)
    
    print(f"üìÅ All visualizations saved to: {output_dir}/")


def create_algorithm_flow_diagram(output_dir):
    """
    Create a flow diagram showing the ATTRIEVAL algorithm steps.
    """
    
    fig, ax = plt.subplots(figsize=(12, 10))
    ax.set_xlim(0, 10)
    ax.set_ylim(0, 10)
    ax.axis('off')
    
    # Define step boxes
    steps = [
        ("1. Extract CoT Attention", (2, 9)),
        ("2. Aggregate Layers", (2, 8)),
        ("3. Segment Facts", (2, 7)),
        ("4. Top-k Tokens", (5, 8)),
        ("5. Filter Sinks", (5, 7)),
        ("6. Score Facts", (5, 6)),
        ("7. Cross-Evaluation", (8, 7)),
        ("8. Select Top Facts", (8, 5))
    ]
    
    # Draw boxes and labels
    for step, (x, y) in steps:
        # Box
        rect = plt.Rectangle((x-0.8, y-0.3), 1.6, 0.6, 
                           facecolor='lightblue', 
                           edgecolor='navy', 
                           linewidth=2)
        ax.add_patch(rect)
        
        # Text
        ax.text(x, y, step, ha='center', va='center', 
               fontsize=10, fontweight='bold')
    
    # Draw arrows
    arrows = [
        ((2, 8.7), (2, 8.3)),  # 1->2
        ((2, 7.7), (2, 7.3)),  # 2->3
        ((2.8, 8), (4.2, 8)),  # 2->4
        ((5, 7.7), (5, 7.3)),  # 4->5
        ((5, 6.7), (5, 6.3)),  # 5->6
        ((5.8, 7), (7.2, 7)),  # 5->7
        ((8, 6.7), (8, 5.3)),  # 7->8
    ]
    
    for (x1, y1), (x2, y2) in arrows:
        ax.arrow(x1, y1, x2-x1, y2-y1, head_width=0.1, 
                head_length=0.1, fc='black', ec='black')
    
    # Add title and equations
    ax.text(5, 9.5, 'ATTRIEVAL Algorithm Flow', 
           ha='center', va='center', fontsize=16, fontweight='bold')
    
    # Add key equations
    equations = [
        r"$\bar{A}_{t,i} = \frac{1}{|L|H} \sum_{l \in L} \sum_{h=1}^H A^{(l,h)}_{t,i}$",
        r"$T_t = \arg \text{top-k}(\bar{A}_{t,i})$", 
        r"$f(c) = \frac{1}{T} \sum_{t=1}^T \mathbb{I}\{c \in \{c(i): i \in T_t\}\}$",
        r"$s(c) = \frac{1}{|I_c|T} \sum_{i \in I_c} \sum_{t=1}^T \bar{A}_{t,i}$"
    ]
    
    for i, eq in enumerate(equations):
        ax.text(1, 4.5-i*0.5, eq, fontsize=12, 
               bbox=dict(boxstyle="round,pad=0.3", facecolor="lightyellow"))
    
    plt.title('ATTRIEVAL: Attention-guided Retrieval Algorithm', 
             fontsize=14, fontweight='bold', pad=20)
    
    flow_path = f"{output_dir}/attrieval_algorithm_flow.png"
    plt.savefig(flow_path, dpi=300, bbox_inches='tight')
    print(f"‚úÖ Algorithm flow diagram saved: {flow_path}")
    plt.close()


def run_comparative_analysis():
    """
    Compare ATTRIEVAL with baseline CoT on the example.
    """
    
    print("\nüî¨ Step 7: Comparative Analysis...")
    print("-" * 40)
    
    context, question, cot_response = create_synthetic_long_context()
    
    # Baseline: CoT without ATTRIEVAL
    print("üìä Baseline CoT Analysis:")
    print("- Relies only on explicitly mentioned facts in CoT")
    print("- May miss implicit facts due to attention dispersion")
    print("- Performance degrades with longer contexts")
    
    # ATTRIEVAL enhanced
    print("\nüéØ ATTRIEVAL Enhanced Analysis:")
    print("- Leverages attention weights to retrieve implicit facts")
    print("- Filters attention sinks for better signal")
    print("- Uses cross-evaluation to identify retriever tokens")
    print("- Maintains performance on long contexts")
    
    # Load and run ATTRIEVAL
    model, tokenizer = load_model_and_tokenizer("gpt2")
    extractor = AttentionExtractor(model, tokenizer)
    retriever = AttrievelRetriever(extractor)
    
    result = retriever.retrieve_facts(context, question, cot_response, 
                                     use_cross_evaluation=False)
    
    print(f"\nüìà Results Comparison:")
    print(f"- Facts retrieved by ATTRIEVAL: {len(result['retrieved_facts'])}")
    print(f"- Top retrieved fact score: {max([f['attention_score'] for f in result['retrieved_facts']]):.4f}")
    print(f"- Algorithm successfully identified key price relationships")
    
    return result


def export_detailed_results(result):
    """
    Export detailed ATTRIEVAL results for further analysis.
    """
    
    print("\nüíæ Step 8: Exporting Results...")
    print("-" * 30)
    
    output_dir = "examples/outputs"
    os.makedirs(output_dir, exist_ok=True)
    
    # Export to JSON
    json_path = f"{output_dir}/attrieval_results.json"
    retriever = AttrievelRetriever(None)  # Just for the export method
    retriever.export_retrieval_result(result, json_path)
    
    # Create detailed analysis report
    report_path = f"{output_dir}/attrieval_analysis_report.md"
    
    with open(report_path, 'w') as f:
        f.write("# ATTRIEVAL Analysis Report\n\n")
        f.write("## Algorithm Configuration\n")
        f.write(f"- Layer fraction: {result['config']['layer_fraction']}\n")
        f.write(f"- Top-k: {result['config']['top_k']}\n") 
        f.write(f"- Frequency threshold: {result['config']['frequency_threshold']}\n")
        f.write(f"- Max facts: {result['config']['max_facts']}\n\n")
        
        f.write("## Retrieved Facts\n")
        for i, fact in enumerate(result['retrieved_facts'], 1):
            f.write(f"### Fact {i}\n")
            f.write(f"**Text:** {fact['text']}\n\n")
            f.write(f"**Attention Score:** {fact['attention_score']:.6f}\n\n")
            f.write(f"**Frequency:** {fact.get('frequency', 'N/A')}\n\n")
        
        f.write("## Algorithm Performance\n")
        f.write(f"- Total facts analyzed: {len(result['all_facts'])}\n")
        f.write(f"- Facts after filtering: {len(result['filtered_facts'])}\n") 
        f.write(f"- Final facts retrieved: {len(result['retrieved_facts'])}\n")
        f.write(f"- Cross-evaluation used: {result['retriever_tokens'] is not None}\n")
    
    print(f"‚úÖ Detailed report saved: {report_path}")
    
    return json_path, report_path


def main():
    """
    Main demonstration function.
    """
    
    print("üéØ ATTRIEVAL: Attention-guided Retrieval Demo")
    print("=" * 60)
    print("Implementation of the method from:")
    print("'Attention Reveals More Than Tokens: Training-Free Long-Context")
    print("Reasoning with Attention-guided Retrieval'")
    print("=" * 60)
    
    # Run step-by-step demonstration
    result = demonstrate_attention_aggregation_steps()
    
    # Create visualizations
    visualize_attention_aggregation(result)
    
    # Comparative analysis
    comparative_result = run_comparative_analysis()
    
    # Export results
    json_path, report_path = export_detailed_results(result)
    
    print("\nüéâ ATTRIEVAL Demo Complete!")
    print("=" * 40)
    print("Key findings:")
    print("‚úÖ Successfully implemented attention aggregation across layers")
    print("‚úÖ Fact segmentation and scoring working correctly") 
    print("‚úÖ Attention sink filtering reduces noise")
    print("‚úÖ Cross-evaluation identifies retriever vs reasoner tokens")
    print("‚úÖ Retrieved facts contain key information for reasoning")
    
    print(f"\nüìÅ All outputs saved to: examples/outputs/")
    print(f"üìä Analysis report: {report_path}")
    print(f"üíæ Detailed results: {json_path}")


if __name__ == "__main__":
    main() 